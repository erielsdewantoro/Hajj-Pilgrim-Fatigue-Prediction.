{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVl-mAbF3YTf",
        "outputId": "8fa2281c-9b3f-43b4-d155-727dd3ddda13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Langkah 1: Menghubungkan Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive berhasil terhubung.\n",
            "\n",
            "Langkah 2: Optimasi Pemuatan Data...\n",
            "Langkah 2a: Membaca header dari /content/drive/MyDrive/Finpro DS/All_Subjects_data_compained3.csv...\n",
            "Kolom yang akan dimuat (dioptimasi): 34\n",
            "Langkah 2c: Memuat 5000000 baris (HANYA kolom yang dibutuhkan)...\n",
            "Data berhasil dimuat. Ukuran: (5000000, 34)\n",
            "\n",
            "Langkah 3: Analisis Data Kunci...\n",
            "Jumlah ID: 17\n",
            "\n",
            "Langkah 3.5: Memulai Pembersihan & Agregasi Data...\n",
            "  - Mengonversi 'createdAt' ke datetime...\n",
            "  - Memaksa kolom sensor menjadi numerik...\n",
            "  - Mengagregasi data (Metode Cerdas)...\n",
            "  - Ukuran data setelah agregasi: (4330715, 34)\n",
            "  - Menyortir data berdasarkan id dan waktu...\n",
            "Pembersihan & Agregasi Selesai.\n",
            "\n",
            "Langkah 4: Preprocessing & Pembuatan Target...\n",
            "Pipeline preprocessing (Imputer + Scaler) siap.\n",
            "\n",
            "Langkah 5: Melakukan Group-Based Split (Split Cerdas)...\n",
            "Data split selesai. Siap melatih 16 peserta, menguji pada ID 0.\n",
            "\n",
            "Langkah 6: Mendefinisikan 3 Model...\n",
            "3 pipeline model (LGBM, LogReg, RF) siap.\n",
            "\n",
            "Langkah 7: Memulai Kontes Model...\n",
            "\n",
            "--- Melatih Model 1: LightGBM ---\n",
            "[LightGBM] [Info] Number of positive: 1300333, number of negative: 2445851\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.575976 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 5199\n",
            "[LightGBM] [Info] Number of data points in the train set: 3746184, number of used features: 29\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n",
            "Selesai dalam 108.51 detik.\n",
            "F1-Score LGBM (Lelah): 0.7209\n",
            "\n",
            "--- Melatih Model 2: Logistic Regression ---\n",
            "Selesai dalam 123.06 detik.\n",
            "F1-Score LogReg (Lelah): 0.0303\n",
            "\n",
            "--- Melatih Model 3: Random Forest ---\n",
            "PERINGATAN: Ini akan memakan waktu lama (bisa 10-20+ menit)...\n",
            "Selesai dalam 626.41 detik.\n",
            "FB1-Score RF (Lelah): 0.1220\n",
            "\n",
            "==================================================\n",
            "============ HASIL AKHIR KONTES MODEL ============\n",
            "==================================================\n",
            "Skor F1 (Kelas 1 'Lelah') - Semakin tinggi semakin baik:\n",
            "  - LGBM                : 0.7209\n",
            "  - RF                  : 0.1220\n",
            "  - LogReg              : 0.0303\n",
            "\n",
            "Waktu Latih - Semakin rendah semakin baik:\n",
            "  - LGBM                : 108.51 detik\n",
            "  - LogReg              : 123.06 detik\n",
            "  - RF                  : 626.41 detik\n",
            "==================================================\n",
            "\n",
            "ðŸ† Model Pemenang (F1-Score Tertinggi): **LGBM**\n",
            "==================================================\n",
            "\n",
            "--- Laporan Klasifikasi Rinci untuk LGBM ---\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "0 (Tidak Lelah)       0.78      0.48      0.60    295961\n",
            "      1 (Lelah)       0.62      0.86      0.72    288570\n",
            "\n",
            "       accuracy                           0.67    584531\n",
            "      macro avg       0.70      0.67      0.66    584531\n",
            "   weighted avg       0.70      0.67      0.66    584531\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "# --- TAMBAHAN IMPORT MODEL ---\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# --- --------------------- ---\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# LANGKAH 1: SETUP LINGKUNGAN\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"Langkah 1: Menghubungkan Google Drive...\")\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive berhasil terhubung.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error menghubungkan Drive: {e}\")\n",
        "    # Berhenti jika drive tidak terhubung\n",
        "    raise\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# LANGKAH 2: PEMUATAN DATA (OPTIMASI RAM + MENGAMBIL 'createdAt')\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\nLangkah 2: Optimasi Pemuatan Data...\")\n",
        "file_path = '/content/drive/MyDrive/Finpro DS/All_Subjects_data_compained3.csv'\n",
        "NROWS_SAMPLE = 5_000_000\n",
        "\n",
        "print(f\"Langkah 2a: Membaca header dari {file_path}...\")\n",
        "try:\n",
        "    df_header = pd.read_csv(file_path, nrows=1)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: File tidak ditemukan di {file_path}\")\n",
        "    raise\n",
        "\n",
        "timestamp_cols = [col for col in df_header.columns if 'Timestamp' in col]\n",
        "COLS_TO_IGNORE = [\n",
        "    'emotionalMoodLevel', 'Unnamed: 0', 'onWristStatus',\n",
        "    'watchStatus', 'rukun',\n",
        "] + timestamp_cols\n",
        "\n",
        "COLS_WE_NEED = [col for col in df_header.columns if col not in COLS_TO_IGNORE]\n",
        "print(f\"Kolom yang akan dimuat (dioptimasi): {len(COLS_WE_NEED)}\")\n",
        "\n",
        "print(f\"Langkah 2c: Memuat {NROWS_SAMPLE} baris (HANYA kolom yang dibutuhkan)...\")\n",
        "try:\n",
        "    df = pd.read_csv(file_path, nrows=NROWS_SAMPLE, usecols=COLS_WE_NEED)\n",
        "    print(f\"Data berhasil dimuat. Ukuran: {df.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saat memuat dengan 'usecols': {e}\")\n",
        "    raise\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# LANGKAH 3: ANALISIS DATA KUNCI\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\nLangkah 3: Analisis Data Kunci...\")\n",
        "print(f\"Jumlah ID: {df['id'].nunique()}\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# LANGKAH 3.5: (PERBAIKAN) AGREGRASI & PEMBERSIHAN\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\nLangkah 3.5: Memulai Pembersihan & Agregasi Data...\")\n",
        "\n",
        "# 3.5a: Konversi Waktu\n",
        "print(\"  - Mengonversi 'createdAt' ke datetime...\")\n",
        "df['createdAt'] = pd.to_datetime(df['createdAt'])\n",
        "\n",
        "# 3.5b: Konversi Paksa Fitur Sensor ke Numerik\n",
        "print(\"  - Memaksa kolom sensor menjadi numerik...\")\n",
        "features_to_check = ['heartRate', 'skinTemperature']\n",
        "for col in features_to_check:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# 3.5c: (FIX 'KeyError') Agregasi Cerdas\n",
        "print(\"  - Mengagregasi data (Metode Cerdas)...\")\n",
        "\n",
        "# Dapatkan SEMUA kolom yang BUKAN 'id' or 'createdAt' (kolom grup kita)\n",
        "cols_to_agg = [col for col in df.columns if col not in ['id', 'createdAt']]\n",
        "\n",
        "# Mulai dengan 'mean' untuk SEMUA kolom\n",
        "agg_dict = {col: 'mean' for col in cols_to_agg}\n",
        "\n",
        "# SEKARANG, ganti (overwrite) 'physicalTiredLevel' ke 'first'\n",
        "# Ini adalah KUNCI agar target kita tidak rusak\n",
        "if 'physicalTiredLevel' in agg_dict:\n",
        "    agg_dict['physicalTiredLevel'] = 'first'\n",
        "else:\n",
        "    # Ini akan memicu error jika tidak dimuat, tapi lebih jelas\n",
        "    print(\"FATAL ERROR: 'physicalTiredLevel' tidak ditemukan di df.columns saat membuat agg_dict.\")\n",
        "    raise KeyError(\"'physicalTiredLevel' not in df.columns\")\n",
        "\n",
        "try:\n",
        "    df = df.groupby(['id', 'createdAt']).agg(agg_dict).reset_index()\n",
        "    print(f\"  - Ukuran data setelah agregasi: {df.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saat agregasi cerdas: {e}\")\n",
        "    raise\n",
        "\n",
        "# 3.5d: SORTING\n",
        "print(\"  - Menyortir data berdasarkan id dan waktu...\")\n",
        "df = df.sort_values(by=['id', 'createdAt'])\n",
        "\n",
        "print(\"Pembersihan & Agregasi Selesai.\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# LANGKAH 4: PREPROCESSING & PEMBUATAN TARGET\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\nLangkah 4: Preprocessing & Pembuatan Target...\")\n",
        "df['y_binary'] = np.where(df['physicalTiredLevel'] > 1, 1, 0)\n",
        "y = df['y_binary']\n",
        "COLS_TO_DROP_FROM_X = ['physicalTiredLevel', 'id', 'y_binary', 'createdAt']\n",
        "cols_to_drop_existing = [col for col in COLS_TO_DROP_FROM_X if col in df.columns]\n",
        "X = df.drop(columns=cols_to_drop_existing)\n",
        "numeric_features = X.columns.tolist()\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[('num', numeric_transformer, numeric_features)],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "print(\"Pipeline preprocessing (Imputer + Scaler) siap.\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# LANGKAH 5: GROUP-BASED SPLIT\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\nLangkah 5: Melakukan Group-Based Split (Split Cerdas)...\")\n",
        "distribusi_per_id = df.groupby('id')['y_binary'].mean()\n",
        "good_test_ids_candidates = distribusi_per_id[(distribusi_per_id > 0.1) & (distribusi_per_id < 0.9)]\n",
        "if good_test_ids_candidates.empty:\n",
        "    good_test_ids_candidates = distribusi_per_id[distribusi_per_id > 0.01]\n",
        "if good_test_ids_candidates.empty:\n",
        "    raise ValueError(\"Tidak ada peserta yang valid untuk test set (data 'Lelah' tidak cukup).\")\n",
        "\n",
        "TEST_ID_CHOICE = good_test_ids_candidates.index[0]\n",
        "TEST_IDS = [TEST_ID_CHOICE]\n",
        "unique_ids = sorted(df['id'].unique())\n",
        "TRAIN_IDS = [id for id in unique_ids if id not in TEST_IDS]\n",
        "\n",
        "train_indices = df['id'].isin(TRAIN_IDS)\n",
        "test_indices = df['id'].isin(TEST_IDS)\n",
        "X_train = X[train_indices]\n",
        "y_train = y[train_indices]\n",
        "X_test = X[test_indices]\n",
        "y_test = y[test_indices]\n",
        "print(f\"Data split selesai. Siap melatih {len(TRAIN_IDS)} peserta, menguji pada ID {TEST_IDS[0]}.\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# LANGKAH 6: DEFINISI 3 MODEL & PIPELINE\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\nLangkah 6: Mendefinisikan 3 Model...\")\n",
        "\n",
        "# Definisikan 3 model. Perhatikan 'class_weight='balanced'' di semua model.\n",
        "model_lgbm = lgb.LGBMClassifier(\n",
        "    random_state=42,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "model_logreg = LogisticRegression(\n",
        "    random_state=42,\n",
        "    class_weight='balanced',\n",
        "    max_iter=1000 # Tambahkan max_iter agar konvergen\n",
        ")\n",
        "\n",
        "model_rf = RandomForestClassifier(\n",
        "    n_estimators=100, # 100 pohon (default)\n",
        "    random_state=42,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1 # Gunakan semua core CPU\n",
        ")\n",
        "\n",
        "# Buat 3 pipeline terpisah, tapi gunakan preprocessor YANG SAMA\n",
        "pipeline_lgbm = Pipeline(steps=[('preprocessor', preprocessor), ('model', model_lgbm)])\n",
        "pipeline_logreg = Pipeline(steps=[('preprocessor', preprocessor), ('model', model_logreg)])\n",
        "pipeline_rf = Pipeline(steps=[('preprocessor', preprocessor), ('model', model_rf)])\n",
        "\n",
        "print(\"3 pipeline model (LGBM, LogReg, RF) siap.\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# LANGKAH 7: MELATIH & MENGEVALUASI SEMUA MODEL\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\nLangkah 7: Memulai Kontes Model...\")\n",
        "\n",
        "# Kamus untuk menyimpan hasil\n",
        "hasil_f1 = {}\n",
        "waktu_latih = {}\n",
        "\n",
        "# --- Model 1: LightGBM ---\n",
        "print(\"\\n--- Melatih Model 1: LightGBM ---\")\n",
        "start_time = time.time()\n",
        "pipeline_lgbm.fit(X_train, y_train)\n",
        "waktu_latih['LGBM'] = time.time() - start_time\n",
        "print(f\"Selesai dalam {waktu_latih['LGBM']:.2f} detik.\")\n",
        "y_pred_lgbm = pipeline_lgbm.predict(X_test)\n",
        "hasil_f1['LGBM'] = f1_score(y_test, y_pred_lgbm, pos_label=1)\n",
        "print(f\"F1-Score LGBM (Lelah): {hasil_f1['LGBM']:.4f}\")\n",
        "\n",
        "# --- Model 2: Logistic Regression ---\n",
        "print(\"\\n--- Melatih Model 2: Logistic Regression ---\")\n",
        "start_time = time.time()\n",
        "pipeline_logreg.fit(X_train, y_train)\n",
        "waktu_latih['LogReg'] = time.time() - start_time\n",
        "print(f\"Selesai dalam {waktu_latih['LogReg']:.2f} detik.\")\n",
        "y_pred_logreg = pipeline_logreg.predict(X_test)\n",
        "hasil_f1['LogReg'] = f1_score(y_test, y_pred_logreg, pos_label=1)\n",
        "print(f\"F1-Score LogReg (Lelah): {hasil_f1['LogReg']:.4f}\")\n",
        "\n",
        "# --- Model 3: Random Forest ---\n",
        "print(\"\\n--- Melatih Model 3: Random Forest ---\")\n",
        "print(\"PERINGATAN: Ini akan memakan waktu lama (bisa 10-20+ menit)...\")\n",
        "start_time = time.time()\n",
        "pipeline_rf.fit(X_train, y_train)\n",
        "waktu_latih['RF'] = time.time() - start_time\n",
        "print(f\"Selesai dalam {waktu_latih['RF']:.2f} detik.\")\n",
        "y_pred_rf = pipeline_rf.predict(X_test)\n",
        "hasil_f1['RF'] = f1_score(y_test, y_pred_rf, pos_label=1)\n",
        "print(f\"FB1-Score RF (Lelah): {hasil_f1['RF']:.4f}\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# LANGKAH 8: PERBANDINGAN FINAL & KEPUTUSAN\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\" HASIL AKHIR KONTES MODEL \".center(50, \"=\"))\n",
        "print(\"=\"*50)\n",
        "print(\"Skor F1 (Kelas 1 'Lelah') - Semakin tinggi semakin baik:\")\n",
        "# Urutkan hasil F1 dari tertinggi ke terendah\n",
        "hasil_f1_sorted = sorted(hasil_f1.items(), key=lambda item: item[1], reverse=True)\n",
        "for model, score in hasil_f1_sorted:\n",
        "    print(f\"  - {model:<20}: {score:.4f}\")\n",
        "\n",
        "print(\"\\nWaktu Latih - Semakin rendah semakin baik:\")\n",
        "waktu_latih_sorted = sorted(waktu_latih.items(), key=lambda item: item[1])\n",
        "for model, wkt in waktu_latih_sorted:\n",
        "    print(f\"  - {model:<20}: {wkt:.2f} detik\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Tentukan pemenang\n",
        "pemenang = hasil_f1_sorted[0][0]\n",
        "print(f\"\\nðŸ† Model Pemenang (F1-Score Tertinggi): **{pemenang}**\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Tampilkan laporan rinci HANYA untuk model pemenang\n",
        "print(f\"\\n--- Laporan Klasifikasi Rinci untuk {pemenang} ---\")\n",
        "if pemenang == 'LGBM':\n",
        "    print(classification_report(y_test, y_pred_lgbm, target_names=['0 (Tidak Lelah)', '1 (Lelah)']))\n",
        "elif pemenang == 'LogReg':\n",
        "    print(classification_report(y_test, y_pred_logreg, target_names=['0 (Tidak Lelah)', '1 (Lelah)']))\n",
        "elif pemenang == 'RF':\n",
        "    print(classification_report(y_test, y_pred_rf, target_names=['0 (Tidak Lelah)', '1 (Lelah)']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Final Project: Analisis Prediktif Kelelahan Jemaah Haji\n",
        "\n",
        "**Nama:** [Eriel Setiawan Dewantoro]"
      ],
      "metadata": {
        "id": "I_4-OmGZBAsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Pendahuluan & Latar Belakang"
      ],
      "metadata": {
        "id": "ceULciWMCnpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Latar Belakang Masalah\n",
        "Kelelahan fisik adalah salah satu risiko kesehatan terbesar yang dihadapi jemaah haji, yang dapat berujung pada insiden kesehatan serius. Deteksi dini kelelahan menjadi krusial. Proyek ini bertujuan memanfaatkan data dari sensor *wearable* (seperti detak jantung, suhu kulit, dan akselerometer) untuk membangun model *machine learning* yang dapat memprediksi tingkat kelelahan jemaah secara *real-time*."
      ],
      "metadata": {
        "id": "KJrTgkzLC8_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Tujuan Proyek (Goal)\n",
        "Tujuan dari notebook ini adalah untuk:\n",
        "1.  **Membersihkan dan Memproses** data sensor mentah yang berukuran sangat besar (jutaan baris).\n",
        "2.  **Membangun & Membandingkan** beberapa model klasifikasi (LGBM, Logistic Regression, Random Forest) untuk memprediksi kelelahan.\n",
        "3.  **Memilih Model Terbaik** berdasarkan metrik F1-Score (karena data tidak seimbang).\n",
        "4.  **Menyajikan Insight** dari model pemenang sebagai dasar pengambilan keputusan."
      ],
      "metadata": {
        "id": "uvvvvuOIDDNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Definisi Target\n",
        "Kita akan menyederhanakan kolom `physicalTiredLevel` (skala 1-4) menjadi masalah **Klasifikasi Biner**:\n",
        "* **Kelas 0 (Tidak Lelah):** `physicalTiledLevel == 1`\n",
        "* **Kelas 1 (Lelah):** `physicalTiledLevel > 1` (yaitu, 2 atau 3)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "U7ZPsjajDEln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Import Library & Setup Lingkungan\n",
        "\n",
        "Sel ini mengimpor semua *library* yang kita butuhkan untuk keseluruhan proyek, mulai dari manipulasi data (Pandas), pemodelan (Scikit-learn, LightGBM), hingga utilitas (time, warnings)."
      ],
      "metadata": {
        "id": "pvCAFn-QC5g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Import Library ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "# Mengabaikan warning agar output bersih\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- Setup Lingkungan & Path ---\n",
        "print(\"Langkah 1: Menghubungkan Google Drive...\")\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive berhasil terhubung.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error menghubungkan Drive: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aIHjkV8BBx3",
        "outputId": "bde03769-5a9a-4f2e-948e-0b03f1de3694"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Langkah 1: Menghubungkan Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive berhasil terhubung.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Pemuatan & Pembersihan Data (Data Loading & Cleaning)"
      ],
      "metadata": {
        "id": "IrfZEMOKBXOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1. Pemuatan Data (Optimasi RAM)\n",
        "Dataset mentah (All_Subjects_data_compained3.csv) berukuran sangat besar (beberapa GB). Untuk mencegah crash RAM di Colab, kami melakukan dua strategi optimasi:\n",
        "\n",
        "Sampling (nrows): Kami hanya akan memuat 5 juta baris pertama sebagai sampel yang representatif.\n",
        "\n",
        "Seleksi Kolom (usecols): Kami membaca header terlebih dahulu dan secara proaktif membuang kolom yang tidak relevan (seperti emotionalMoodLevel, Unnamed: 0, dan berbagai Timestamp individual) sebelum data dimuat ke memori."
      ],
      "metadata": {
        "id": "-avW5TwXDc6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Tentukan Path & Variabel ---\n",
        "print(\"\\nLangkah 2: Optimasi Pemuatan Data...\")\n",
        "file_path = '/content/drive/MyDrive/Finpro DS/All_Subjects_data_compained3.csv'\n",
        "NROWS_SAMPLE = 5_000_000\n",
        "\n",
        "# 2a: Baca header untuk seleksi kolom\n",
        "try:\n",
        "    df_header = pd.read_csv(file_path, nrows=1)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: File tidak ditemukan di {file_path}\")\n",
        "    raise\n",
        "\n",
        "# Tentukan kolom yang akan diabaikan saat pemuatan\n",
        "timestamp_cols = [col for col in df_header.columns if 'Timestamp' in col]\n",
        "COLS_TO_IGNORE = [\n",
        "    'emotionalMoodLevel', 'Unnamed: 0', 'onWristStatus',\n",
        "    'watchStatus', 'rukun',\n",
        "] + timestamp_cols\n",
        "\n",
        "# Tentukan kolom yang kita BUTUHKAN\n",
        "COLS_WE_NEED = [col for col in df_header.columns if col not in COLS_TO_IGNORE]\n",
        "print(f\"Kolom yang akan dimuat (dioptimasi): {len(COLS_WE_NEED)}\")\n",
        "\n",
        "# 2c: Muat data dengan optimasi\n",
        "try:\n",
        "    df = pd.read_csv(file_path, nrows=NROWS_SAMPLE, usecols=COLS_WE_NEED)\n",
        "    print(f\"Data berhasil dimuat. Ukuran: {df.shape}\")\n",
        "    print(f\"Jumlah ID peserta unik: {df['id'].nunique()}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saat memuat dengan 'usecols': {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bFxRCCcBYgv",
        "outputId": "d5a7ce39-46ae-4955-dd56-f7232f8d7749"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Langkah 2: Optimasi Pemuatan Data...\n",
            "Kolom yang akan dimuat (dioptimasi): 34\n",
            "Data berhasil dimuat. Ukuran: (5000000, 34)\n",
            "Jumlah ID peserta unik: 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2. Pembersihan & Agregasi Data (Kunci Peningkatan Performa)\n",
        "Data sensor mentah seringkali \"berisik\" (noisy). menemukan bahwa ada banyak timestamp duplikat untuk peserta yang sama (misalnya, beberapa sensor melaporkan data pada detik yang sama).\n",
        "\n",
        "Untuk membersihkan ini dan menciptakan dataset yang stabil,  melakukan Agregasi Cerdas:\n",
        "\n",
        "Saaya mengelompokkan data berdasarkan id dan createdAt.\n",
        "\n",
        "Untuk semua fitur sensor (heartRate, skinTemperature, dll.), mengambil nilai rata-rata (mean).\n",
        "\n",
        "Untuk kolom target (physicalTiledLevel), mengambil nilai pertama (first) agar datanya tidak rusak (misal, mean dari 1 dan 2 menjadi 1.5).\n",
        "\n",
        "Langkah ini terbukti sangat krusial dan meningkatkan F1-Score kami secara signifikan (dari ~0.64 ke ~0.72 dalam eksperimen awal)."
      ],
      "metadata": {
        "id": "xi4dxJJiBfs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLangkah 3.5: Memulai Pembersihan & Agregasi Data...\")\n",
        "\n",
        "# 3.5a: Konversi Waktu\n",
        "print(\"  - Mengonversi 'createdAt' ke datetime...\")\n",
        "df['createdAt'] = pd.to_datetime(df['createdAt'])\n",
        "\n",
        "# 3.5b: Konversi Paksa Fitur Sensor ke Numerik\n",
        "print(\"  - Memaksa kolom sensor menjadi numerik...\")\n",
        "features_to_check = ['heartRate', 'skinTemperature']\n",
        "for col in features_to_check:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# 3.5c: Agregasi Cerdas (FIX 'KeyError')\n",
        "print(\"  - Mengagregasi data (Metode Cerdas)...\")\n",
        "# Correct the column name here from 'physicalTiledLevel' to 'physicalTiredLevel'\n",
        "if 'physicalTiredLevel' not in df.columns:\n",
        "    raise KeyError(\"'physicalTiredLevel' not loaded from CSV.\")\n",
        "\n",
        "feature_cols = [\n",
        "    col for col in df.columns\n",
        "    if col not in ['id', 'createdAt', 'physicalTiredLevel'] # Corrected column name\n",
        "]\n",
        "agg_dict = {col: 'mean' for col in feature_cols}\n",
        "agg_dict['physicalTiredLevel'] = 'first' # Corrected column name\n",
        "\n",
        "try:\n",
        "    df = df.groupby(['id', 'createdAt']).agg(agg_dict).reset_index()\n",
        "    print(f\"  - Ukuran data setelah agregasi: {df.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saat agregasi cerdas: {e}\")\n",
        "    raise\n",
        "\n",
        "# 3.5d: SORTING\n",
        "print(\"  - Menyortir data berdasarkan id dan waktu...\")\n",
        "df = df.sort_values(by=['id', 'createdAt'])\n",
        "print(\"Pembersihan & Agregasi Selesai.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cC5-FlzBglg",
        "outputId": "fd0a62d5-a781-4f96-b69d-6ce782bf567f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Langkah 3.5: Memulai Pembersihan & Agregasi Data...\n",
            "  - Mengonversi 'createdAt' ke datetime...\n",
            "  - Memaksa kolom sensor menjadi numerik...\n",
            "  - Mengagregasi data (Metode Cerdas)...\n",
            "  - Ukuran data setelah agregasi: (4330715, 34)\n",
            "  - Menyortir data berdasarkan id dan waktu...\n",
            "Pembersihan & Agregasi Selesai.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.Definisi Target & Pipeline Pra-pemrosesan"
      ],
      "metadata": {
        "id": "sf1Qdhe1BjQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.1. Pembuatan Target (X) dan Fitur (y)\n",
        "Kami sekarang membuat target biner y_binary dan memisahkan fitur X (semua data sensor) dari data meta (seperti id, createdAt, dan kolom target)."
      ],
      "metadata": {
        "id": "22digD7tD2EE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.2. Pipeline Pra-pemrosesan\n",
        "Kami membangun Pipeline scikit-learn untuk mengotomatisasi persiapan data. Pipeline ini akan:\n",
        "\n",
        "SimpleImputer: Mengisi missing values (NaN) yang mungkin ada di data sensor dengan nilai median dari kolom tersebut.\n",
        "\n",
        "StandardScaler: Men-skala semua fitur (misal, heartRate 60-150 dan skinTemperature 30-40) ke dalam rentang yang sama agar model tidak bias."
      ],
      "metadata": {
        "id": "iedXdKB8D3em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLangkah 4: Preprocessing & Pembuatan Target...\")\n",
        "# 4a: Buat target biner y\n",
        "df['y_binary'] = np.where(df['physicalTiredLevel'] > 1, 1, 0)\n",
        "y = df['y_binary']\n",
        "\n",
        "# 4b: Buat fitur X\n",
        "COLS_TO_DROP_FROM_X = ['physicalTiredLevel', 'id', 'y_binary', 'createdAt']\n",
        "cols_to_drop_existing = [col for col in COLS_TO_DROP_FROM_X if col in df.columns]\n",
        "X = df.drop(columns=cols_to_drop_existing)\n",
        "print(f\"Fitur (X) dibuat dengan {X.shape[1]} kolom.\")\n",
        "\n",
        "# 4c: Buat pipeline preprocessing\n",
        "numeric_features = X.columns.tolist()\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[('num', numeric_transformer, numeric_features)],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "print(\"Pipeline preprocessing (Imputer + Scaler) siap.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0FNMdFmBo9A",
        "outputId": "c1d11864-c71e-4e97-bb7e-f99b7b61fbfe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Langkah 4: Preprocessing & Pembuatan Target...\n",
            "Fitur (X) dibuat dengan 31 kolom.\n",
            "Pipeline preprocessing (Imputer + Scaler) siap.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.Metodologi Validasi (Group-Based Split)\n",
        "Ini adalah langkah paling krusial dalam proyek ini.\n",
        "\n",
        "Masalah: Kita tidak bisa menggunakan train_test_split standar. Jika kita melakukannya, data dari Peserta A jam 10:00 bisa masuk ke train set dan data dari Peserta A jam 10:01 bisa masuk ke test set. Ini menyebabkan Data Leakage (kebocoran data) dan model akan mendapatkan skor F1 yang tinggi secara artifisial, namun akan gagal di dunia nyata.\n",
        "\n",
        "Solusi: Group-Based Split (Pemisahan Berbasis Grup). Kami mensimulasikan skenario dunia nyata: \"Bisakah model yang dilatih pada 16 peserta, memprediksi kelelahan pada peserta ke-17 yang baru?\"\n",
        "\n",
        "Train Set: Data dari 16 peserta (ID 1-16).\n",
        "\n",
        "Test Set: Data dari 1 peserta (ID 0), yang kami pilih secara dinamis karena memiliki distribusi target yang seimbang (tidak 100% 'Lelah' atau 100% 'Tidak Lelah')."
      ],
      "metadata": {
        "id": "TN6abE1ABqSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLangkah 5: Melakukan Group-Based Split (Split Cerdas)...\")\n",
        "distribusi_per_id = df.groupby('id')['y_binary'].mean()\n",
        "\n",
        "# Cari ID yang 'ideal' untuk testing (punya campuran 0 dan 1)\n",
        "good_test_ids_candidates = distribusi_per_id[(distribusi_per_id > 0.1) & (distribusi_per_id < 0.9)]\n",
        "if good_test_ids_candidates.empty:\n",
        "    good_test_ids_candidates = distribusi_per_id[distribusi_per_id > 0.01]\n",
        "if good_test_ids_candidates.empty:\n",
        "    raise ValueError(\"Tidak ada peserta yang valid untuk test set (data 'Lelah' tidak cukup).\")\n",
        "\n",
        "# Pilih kandidat pertama sebagai Test ID\n",
        "TEST_ID_CHOICE = good_test_ids_candidates.index[0]\n",
        "TEST_IDS = [TEST_ID_CHOICE]\n",
        "unique_ids = sorted(df['id'].unique())\n",
        "TRAIN_IDS = [id for id in unique_ids if id not in TEST_IDS]\n",
        "\n",
        "# Terapkan split\n",
        "train_indices = df['id'].isin(TRAIN_IDS)\n",
        "test_indices = df['id'].isin(TEST_IDS)\n",
        "X_train = X[train_indices]\n",
        "y_train = y[train_indices]\n",
        "X_test = X[test_indices]\n",
        "y_test = y[test_indices]\n",
        "print(f\"Data split selesai. Siap melatih {len(TRAIN_IDS)} peserta, menguji pada ID {TEST_IDS[0]}.\")\n",
        "print(f\"Ukuran Train Set: {X_train.shape[0]} baris\")\n",
        "print(f\"Ukuran Test Set:  {X_test.shape[0]} baris\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO-M23IhBt7x",
        "outputId": "f5f95d44-186f-4155-96f4-8364586dfc9b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Langkah 5: Melakukan Group-Based Split (Split Cerdas)...\n",
            "Data split selesai. Siap melatih 16 peserta, menguji pada ID 0.\n",
            "Ukuran Train Set: 3746184 baris\n",
            "Ukuran Test Set:  584531 baris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.Pemodelan & Kontes Model\n",
        "Sekarang kami siap melatih model. Sesuai penugasan, kami akan membandingkan 3 model klasifikasi populer:\n",
        "\n",
        "Logistic Regression: Model linear sederhana, bagus sebagai baseline.\n",
        "\n",
        "Random Forest: Model ensemble berbasis tree yang kuat dan populer.\n",
        "\n",
        "LightGBM (LGBM): Model gradient boosting modern yang dikenal cepat dan sangat akurat.\n",
        "\n",
        "Untuk semua model, kami menggunakan class_weight='balanced' untuk menangani data yang tidak seimbang."
      ],
      "metadata": {
        "id": "T98VdMsQBwWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLangkah 6: Mendefinisikan 3 Model...\")\n",
        "\n",
        "# Definisikan 3 model\n",
        "model_lgbm = lgb.LGBMClassifier(random_state=42, class_weight='balanced')\n",
        "model_logreg = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
        "model_rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)\n",
        "\n",
        "# Buat 3 pipeline terpisah (menggunakan preprocessor yang sama)\n",
        "pipeline_lgbm = Pipeline(steps=[('preprocessor', preprocessor), ('model', model_lgbm)])\n",
        "pipeline_logreg = Pipeline(steps=[('preprocessor', preprocessor), ('model', model_logreg)])\n",
        "pipeline_rf = Pipeline(steps=[('preprocessor', preprocessor), ('model', model_rf)])\n",
        "\n",
        "print(\"3 pipeline model (LGBM, LogReg, RF) siap.\")\n",
        "\n",
        "# --- Mulai Kontes Model ---\n",
        "print(\"\\nLangkah 7: Memulai Kontes Model...\")\n",
        "hasil_f1 = {}\n",
        "waktu_latih = {}\n",
        "\n",
        "# --- Model 1: LightGBM ---\n",
        "print(\"\\n--- Melatih Model 1: LightGBM ---\")\n",
        "start_time = time.time()\n",
        "pipeline_lgbm.fit(X_train, y_train)\n",
        "waktu_latih['LGBM'] = time.time() - start_time\n",
        "print(f\"Selesai dalam {waktu_latih['LGBM']:.2f} detik.\")\n",
        "y_pred_lgbm = pipeline_lgbm.predict(X_test)\n",
        "hasil_f1['LGBM'] = f1_score(y_test, y_pred_lgbm, pos_label=1)\n",
        "print(f\"F1-Score LGBM (Lelah): {hasil_f1['LGBM']:.4f}\")\n",
        "\n",
        "# --- Model 2: Logistic Regression ---\n",
        "print(\"\\n--- Melatih Model 2: Logistic Regression ---\")\n",
        "start_time = time.time()\n",
        "pipeline_logreg.fit(X_train, y_train)\n",
        "waktu_latih['LogReg'] = time.time() - start_time\n",
        "print(f\"Selesai dalam {waktu_latih['LogReg']:.2f} detik.\")\n",
        "y_pred_logreg = pipeline_logreg.predict(X_test)\n",
        "hasil_f1['LogReg'] = f1_score(y_test, y_pred_logreg, pos_label=1)\n",
        "print(f\"F1-Score LogReg (Lelah): {hasil_f1['LogReg']:.4f}\")\n",
        "\n",
        "# --- Model 3: Random Forest ---\n",
        "print(\"\\n--- Melatih Model 3: Random Forest ---\")\n",
        "print(\"PERINGATAN: Ini akan memakan waktu lama (bisa 10-20+ menit)...\")\n",
        "start_time = time.time()\n",
        "pipeline_rf.fit(X_train, y_train)\n",
        "waktu_latih['RF'] = time.time() - start_time\n",
        "print(f\"Selesai dalam {waktu_latih['RF']:.2f} detik.\")\n",
        "y_pred_rf = pipeline_rf.predict(X_test)\n",
        "hasil_f1['RF'] = f1_score(y_test, y_pred_rf, pos_label=1)\n",
        "print(f\"F1-Score RF (Lelah): {hasil_f1['RF']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bmK0RjCB65m",
        "outputId": "6235687a-516d-43b4-d0c2-14e7f06eccbc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Langkah 6: Mendefinisikan 3 Model...\n",
            "3 pipeline model (LGBM, LogReg, RF) siap.\n",
            "\n",
            "Langkah 7: Memulai Kontes Model...\n",
            "\n",
            "--- Melatih Model 1: LightGBM ---\n",
            "[LightGBM] [Info] Number of positive: 1300333, number of negative: 2445851\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.786865 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 5199\n",
            "[LightGBM] [Info] Number of data points in the train set: 3746184, number of used features: 29\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n",
            "Selesai dalam 108.22 detik.\n",
            "F1-Score LGBM (Lelah): 0.7209\n",
            "\n",
            "--- Melatih Model 2: Logistic Regression ---\n",
            "Selesai dalam 124.88 detik.\n",
            "F1-Score LogReg (Lelah): 0.0303\n",
            "\n",
            "--- Melatih Model 3: Random Forest ---\n",
            "PERINGATAN: Ini akan memakan waktu lama (bisa 10-20+ menit)...\n",
            "Selesai dalam 601.96 detik.\n",
            "F1-Score RF (Lelah): 0.1220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.Hasil & Evaluasi Model\n",
        "Setelah ketiga model selesai dilatih, kami membandingkan performa F1-Score (untuk kelas 'Lelah') dan waktu pelatihannya."
      ],
      "metadata": {
        "id": "g6JP7oDWCIsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\" HASIL AKHIR KONTES MODEL \".center(50, \"=\"))\n",
        "print(\"=\"*50)\n",
        "print(\"Skor F1 (Kelas 1 'Lelah') - Semakin tinggi semakin baik:\")\n",
        "# Urutkan hasil F1 dari tertinggi ke terendah\n",
        "hasil_f1_sorted = sorted(hasil_f1.items(), key=lambda item: item[1], reverse=True)\n",
        "for model, score in hasil_f1_sorted:\n",
        "    print(f\"  - {model:<20}: {score:.4f}\")\n",
        "\n",
        "print(\"\\nWaktu Latih - Semakin rendah semakin baik:\")\n",
        "waktu_latih_sorted = sorted(waktu_latih.items(), key=lambda item: item[1])\n",
        "for model, wkt in waktu_latih_sorted:\n",
        "    print(f\"  - {model:<20}: {wkt:.2f} detik\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Tentukan pemenang\n",
        "pemenang = hasil_f1_sorted[0][0]\n",
        "print(f\"\\nðŸ† Model Pemenang (F1-Score Tertinggi): **{pemenang}**\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Tampilkan laporan rinci HANYA untuk model pemenang\n",
        "print(f\"\\n--- Laporan Klasifikasi Rinci untuk {pemenang} ---\")\n",
        "if pemenang == 'LGBM':\n",
        "    print(classification_report(y_test, y_pred_lgbm, target_names=['0 (Tidak Lelah)', '1 (Lelah)']))\n",
        "elif pemenang == 'LogReg':\n",
        "    print(classification_report(y_test, y_pred_logreg, target_names=['0 (Tidak Lelah)', '1 (Lelah)']))\n",
        "elif pemenang == 'RF':\n",
        "    print(classification_report(y_test, y_pred_rf, target_names=['0 (Tidak Lelah)', '1 (Lelah)']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVpMQfAzCJdg",
        "outputId": "22db4b71-6133-4881-c7d8-58b7dbd4d030"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "============ HASIL AKHIR KONTES MODEL ============\n",
            "==================================================\n",
            "Skor F1 (Kelas 1 'Lelah') - Semakin tinggi semakin baik:\n",
            "  - LGBM                : 0.7209\n",
            "  - RF                  : 0.1220\n",
            "  - LogReg              : 0.0303\n",
            "\n",
            "Waktu Latih - Semakin rendah semakin baik:\n",
            "  - LGBM                : 108.22 detik\n",
            "  - LogReg              : 124.88 detik\n",
            "  - RF                  : 601.96 detik\n",
            "==================================================\n",
            "\n",
            "ðŸ† Model Pemenang (F1-Score Tertinggi): **LGBM**\n",
            "==================================================\n",
            "\n",
            "--- Laporan Klasifikasi Rinci untuk LGBM ---\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "0 (Tidak Lelah)       0.78      0.48      0.60    295961\n",
            "      1 (Lelah)       0.62      0.86      0.72    288570\n",
            "\n",
            "       accuracy                           0.67    584531\n",
            "      macro avg       0.70      0.67      0.66    584531\n",
            "   weighted avg       0.70      0.67      0.66    584531\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8.Analisis Hasil, Insight, & Keputusan\n",
        "8.1. Analisis Hasil Kontes\n",
        "Hasil kontes sangat jelas:\n",
        "\n",
        "LightGBM (LGBM) (F1: ~0.72): Menjadi pemenang mutlak. Model ini mampu menemukan pola kompleks dalam data sensor dan memiliki waktu latih yang sangat cepat (jauh lebih cepat dari Random Forest).\n",
        "\n",
        "Random Forest (RF) (F1: ~0.12): Berperforma sangat buruk. Ini menunjukkan bahwa strategi bagging standar (voting) dari RF tidak efektif untuk data ini.\n",
        "\n",
        "Logistic Regression (LogReg) (F1: ~0.03): Gagal total. Ini adalah insight penting yang membuktikan bahwa hubungan antara data sensor (detak jantung, suhu) dan kelelahan bukanlah masalah linear.\n",
        "\n",
        "8.2. Analisis Model Pemenang (LGBM)\n",
        "Melihat laporan klasifikasi rinci untuk LGBM:\n",
        "\n",
        "                 precision    recall  f1-score   support\n",
        "0 (Tidak Lelah)       0.78      0.48      0.60    295961\n",
        "      1 (Lelah)       0.62      0.86      0.72    288570\n",
        "Insight Kunci (Recall Tinggi): Model ini memiliki Recall 0.86 untuk kelas 'Lelah'.\n",
        "\n",
        "Apa Artinya: Dari semua jemaah yang sebenarnya Lelah, model kita berhasil mengidentifikasi 86% dari mereka. Ini adalah hasil yang fantastis untuk kasus penggunaan medis.\n",
        "\n",
        "Trade-off (Precision Rendah): Precision-nya 0.62, yang berarti ada beberapa \"alarm palsu\" (false positive).\n",
        "\n",
        "8.3. Keputusan Bisnis & Kesimpulan\n",
        "Keputusan: Model LightGBM (LGBM) dipilih sebagai model produksi.\n",
        "\n",
        "Alasan:\n",
        "\n",
        "Model ini memiliki F1-Score (0.72) tertinggi, menunjukkan keseimbangan terbaik antara precision dan recall.\n",
        "\n",
        "Secara kritis, Recall yang tinggi (0.86) adalah metrik yang paling penting untuk masalah ini. Dalam konteks kesehatan jemaah, kita lebih memilih memiliki beberapa \"alarm palsu\" (Precision rendah) daripada melewatkan satu jemaah yang benar-benar kelelahan (Recall rendah).\n",
        "\n",
        "Model ini juga sangat efisien, melatih data 3.7 juta baris jauh lebih cepat daripada Random Forest.\n",
        "\n",
        "Proyek ini berhasil membangun pipeline yang valid dan robust untuk membersihkan data sensor dan melatih model yang mampu memprediksi kelelahan jemaah dengan akurasi yang dapat diandalkan (F1 0.72) menggunakan metodologi anti-kebocoran data."
      ],
      "metadata": {
        "id": "izO1Iws4CP1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Simpan Aset Model untuk Produksi\n",
        "Sebagai langkah terakhir, kita bisa menyimpan model pemenang (seluruh pipeline) dan data bersih kita untuk digunakan di aplikasi web (Streamlit) atau dashboard (Power BI)."
      ],
      "metadata": {
        "id": "_2b1uydNCYKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "print(\"\\nLangkah 9: Menyimpan Aset Model Juara (LGBM)...\")\n",
        "BASE_PATH = '/content/drive/MyDrive/Finpro DS/'\n",
        "\n",
        "# 1. Simpan Model/Pipeline (File .pkl)\n",
        "model_file_path = f'{BASE_PATH}model_pemenang_LGBM.pkl'\n",
        "try:\n",
        "    # Kita simpan pipeline_lgbm yang sudah dilatih\n",
        "    joblib.dump(pipeline_lgbm, model_file_path)\n",
        "    print(f\"âœ… Model (pipeline) berhasil disimpan di: {model_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error menyimpan model: {e}\")\n",
        "\n",
        "# 2. Simpan Data Bersih (File .parquet)\n",
        "data_file_path = f'{BASE_PATH}data_bersih_agregasi.parquet'\n",
        "try:\n",
        "    # Simpan 'df' (data bersih 4.3 juta baris) ke format Parquet\n",
        "    df.to_parquet(data_file_path, index=False)\n",
        "    print(f\"âœ… Data bersih (Parquet) berhasil disimpan di: {data_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error menyimpan data Parquet: {e}\")\n",
        "\n",
        "print(\"\\n--- PROYEK SELESAI ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODtFdc3QCZBe",
        "outputId": "becbbc11-9ed1-4163-aecd-5123cde2bbca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Langkah 9: Menyimpan Aset Model Juara (LGBM)...\n",
            "âœ… Model (pipeline) berhasil disimpan di: /content/drive/MyDrive/Finpro DS/model_pemenang_LGBM.pkl\n",
            "âœ… Data bersih (Parquet) berhasil disimpan di: /content/drive/MyDrive/Finpro DS/data_bersih_agregasi.parquet\n",
            "\n",
            "--- PROYEK SELESAI ---\n"
          ]
        }
      ]
    }
  ]
}